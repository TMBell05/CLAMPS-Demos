{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0197313",
   "metadata": {},
   "source": [
    "# CLAMPS Lidar VAD Tutorial\n",
    "\n",
    "This tutorial will show you how to work with PPI data from the Halo Photonics Streamline lidars with the CLAMPS systems at OU and NSSL.\n",
    "\n",
    "## Getting Data\n",
    "CLAMPS data is stored on a THREDDS server (https://data.nssl.noaa.gov/thredds/catalog/FRDD/CLAMPS/catalog.html). We are going to use Siphon to grab the data from the server. If you use Anaconda, Siphon can be download by running\n",
    "```\n",
    "conda install -c conda-forge siphon\n",
    "```\n",
    "on the command line. If you use pip the command would be\n",
    "```\n",
    "pip install siphon\n",
    "```\n",
    "\n",
    "## Wind Profiles from VADs\n",
    "Velocity Azimuth Display (VADs) are one of the most common scan types from the lidars that you will likely be working with. A VAD scan is performed at a constant elevation angle while scanning at different points in azimuth (sort of like a radar). From these VADs you can obtain low-level wind profiles by performing a VAD analysis. If the environment is homogeneous and the wind is constant during the scanning time, the radial velocities from a VAD will fit a sine wave. From this sine wave fit, the u, v, and w wind components can be determined.\n",
    "\n",
    "We are going to work with VAD scans from the CLAMPS 2 lidar during the BLISS-FUL campaign on June 20, 2021. Let's first get the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ff7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from siphon.catalog import TDSCatalog\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Catalog for the CLAMPS2 ingested VAD data\n",
    "catURL = \"https://data.nssl.noaa.gov/thredds/catalog/FRDD/CLAMPS/clamps/clamps2/ingested/clampsdlppiC2.b1/catalog.xml\"\n",
    "\n",
    "# Open the catalog\n",
    "cat = TDSCatalog(catURL)\n",
    "\n",
    "# Date we want to grab\n",
    "dt = datetime(2021, 6, 20)\n",
    "\n",
    "# Get the dates for all the netCDF datasets (have to do it this way because file names can be messed up)\n",
    "nc_dates = []\n",
    "for ds in cat.datasets:\n",
    "    if '.cdf' in ds:\n",
    "        try:\n",
    "            nc_dates.append(datetime.strptime(ds, \"clampsdlppiC2.b1.%Y%m%d.%H%M%S.cdf\"))\n",
    "        except:\n",
    "            nc_dates.append(datetime(2100,1,1))\n",
    "            print('This file name is messed up: ' + ds)\n",
    "\n",
    "nc_dates = np.array(nc_dates)\n",
    "\n",
    "# Find the index of the date we want\n",
    "ind = np.argmin(np.abs(dt - nc_dates))\n",
    "\n",
    "# Get the dataset\n",
    "ds = cat.datasets[ind]\n",
    "\n",
    "# Download the dataset we identified to our current working directory\n",
    "ds.download()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c969b7",
   "metadata": {},
   "source": [
    "The data is now downloaded to your working directory. It is a netCDF file so we are going to open it with the netCDF4 package and look at the variables in the netCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6864c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "\n",
    "nc = Dataset(ds,'r')\n",
    "\n",
    "print(nc.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03530734",
   "metadata": {},
   "source": [
    "We want to read in the hour, azimuth, elevation, scan number, velocity, intensity, and range variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5079a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour = nc.variables['hour'][:]\n",
    "azimuth = nc.variables['azimuth'][:]\n",
    "elevation = nc.variables['elevation'][:]\n",
    "snum = nc.variables['snum'][:]\n",
    "velocity = nc.variables['velocity'][:]\n",
    "intensity = nc.variables['intensity'][:]\n",
    "rng = nc.variables['range'][:]\n",
    "nc.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e59bb8",
   "metadata": {},
   "source": [
    "The data is not currently separated by scan. We need to use the scan number field to find the unique scans and seperate them so we can do the VAD calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7a3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique returns sorted indices. We don't want that, so we are going \n",
    "# to only use the return_index option and make an unsorted unique array\n",
    "indexes = np.unique(snum, return_index=True)[1]\n",
    "u_snum = [snum[index] for index in sorted(indexes)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72592c49",
   "metadata": {},
   "source": [
    "Let's plot the radial velocity by azimuth for a scan random scan at a range of 500 m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2c876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Picking a random scan\n",
    "foo = np.where(u_snum[750] == snum)\n",
    "\n",
    "# Now find the range closest to 500 m\n",
    "fah = np.where(np.nanmin(np.abs(rng-0.5)) == np.abs(rng-0.5))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(azimuth[foo],velocity[foo[0],fah[0]])\n",
    "plt.xlim(0,365)\n",
    "plt.ylim(-10,10)\n",
    "plt.ylabel('Vr [m/s]')\n",
    "plt.xlabel('Deg')\n",
    "plt.title('Hour: %2.2f UTC' % (np.nanmean(hour[foo[0]])))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16e5ba3",
   "metadata": {},
   "source": [
    "You can see here how the radial velocity at each azimuth in the VAD has a sine/cosine wave shape. The amplitude of the wave is related to the wind speed, the phase of the wave is related to the wind direction and the offset from zero is related to the vertical velocity. Note how the radial velocities do not form a perfect wave, this can be due to things like measurement error or a heterogeneous environment. The above plot is for a scan that occurred at night when the environment is often more homogeneous. Let's look at what this plot looks like in the daytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf41c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking a random scan\n",
    "foo = np.where(u_snum[1862] == snum)\n",
    "\n",
    "# Now find the range closest to 500 m\n",
    "fah = np.where(np.nanmin(np.abs(rng-0.5)) == np.abs(rng-0.5))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(azimuth[foo],velocity[foo[0],fah[0]])\n",
    "plt.xlim(0,365)\n",
    "plt.ylim(-10,10)\n",
    "plt.ylabel('Vr [m/s]')\n",
    "plt.xlabel('Deg')\n",
    "plt.title('Hour: %2.2f UTC' % (np.nanmean(hour[foo[0]])))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0daa9d8",
   "metadata": {},
   "source": [
    "For this scan, the wave is more distorted so a VAD analysis is going to be less accurate. Most of the time, VAD analysis is going more accurate in stable boundary layers than convective boundary layers.\n",
    "\n",
    "Now we are going to calculate VAD winds for the entire file. A discription of the function that we are going to use can be found [here](https://doi.org/10.2172/1238069). This is a least-squares based approach where we are minimizing the error between the retrieved wind projected into radial velocity space and the observed radial velocities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b008a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ARM_VAD(vr,ranges,el,az):\n",
    "    \n",
    "    # Need these x, y, and z distances for calculating the residuals\n",
    "    x = ranges[None,:]*np.cos(np.radians(el))*np.sin(np.radians(az[:,None]))\n",
    "    y = ranges[None,:]*np.cos(np.radians(el))*np.cos(np.radians(az[:,None]))\n",
    "    z = ranges*np.sin(np.radians(el))\n",
    "    \n",
    "    \n",
    "    # Initialize the arrays\n",
    "    u = np.ones(len(ranges))*np.nan\n",
    "    v = np.ones(len(ranges))*np.nan\n",
    "    w = np.ones(len(ranges))*np.nan\n",
    "    du = np.ones(len(ranges))*np.nan\n",
    "    dv = np.ones(len(ranges))*np.nan\n",
    "    dw = np.ones(len(ranges))*np.nan\n",
    "    \n",
    "    # Loop over all the ranges\n",
    "    for i in range(len(ranges)):\n",
    "        # Search for missing data\n",
    "        foo = np.where(~np.isnan(vr[:,i]))[0]\n",
    "\n",
    "        # Need at least 25% of the azimuth radial velocities available for retrieval\n",
    "        if len(foo) <= len(az)/4:\n",
    "            u[i] = np.nan\n",
    "            v[i] = np.nan\n",
    "            w[i] = np.nan\n",
    "            continue\n",
    "\n",
    "        # Calculate the different portions of the A matrix\n",
    "        A11 = (np.cos(np.deg2rad(el))**2) * np.sum(np.sin(np.deg2rad(az[foo]))**2)\n",
    "        A12 = (np.cos(np.deg2rad(el))**2) * np.sum(np.sin(np.deg2rad(az[foo])) *\\\n",
    "                np.cos(np.deg2rad(az[foo])))\n",
    "        A13 = (np.cos(np.deg2rad(el))*np.sin(np.deg2rad(el))) *\\\n",
    "                np.sum(np.sin(np.deg2rad(az[foo])))\n",
    "        A22 = (np.cos(np.deg2rad(el))**2) * np.sum(np.cos(np.deg2rad(az[foo]))**2)\n",
    "        A23 = (np.cos(np.deg2rad(el))*np.sin(np.deg2rad(el))) *\\\n",
    "                np.sum(np.cos(np.deg2rad(az[foo])))\n",
    "        A33 = len(az[foo]) * (np.sin(np.deg2rad(el))**2)\n",
    "\n",
    "        # Construct the A matrix\n",
    "        A = np.array([[A11,A12,A13],[A12,A22,A23],[A13,A23,A33]])\n",
    "        \n",
    "        # Invert the A matrix\n",
    "        invA = np.linalg.inv(A)\n",
    "    \n",
    "        # The digagonals of invA give the uncertainties for each wind component\n",
    "        du[i] = np.sqrt(invA[0,0])\n",
    "        dv[i] = np.sqrt(invA[1,1])\n",
    "        dw[i] = np.sqrt(invA[2,2])\n",
    "    \n",
    "        # Calculate the different portions of the b vector\n",
    "        b1 = np.cos(np.deg2rad(el)) * np.sum(vr[foo,i] *\\\n",
    "            np.sin(np.deg2rad(az[foo])))\n",
    "        b2 = np.cos(np.deg2rad(el)) * np.sum(vr[foo,i] *\\\n",
    "            np.cos(np.deg2rad(az[foo])))\n",
    "        b3 = np.sin(np.deg2rad(el)) * np.sum(vr[foo,i])\n",
    "    \n",
    "        # Constuct the b vector\n",
    "        b = np.array([b1,b2,b3])\n",
    "    \n",
    "        # Do the matrix math\n",
    "        temp = invA.dot(b)\n",
    "        u[i] = temp[0]\n",
    "        v[i] = temp[1]\n",
    "        w[i] = temp[2]\n",
    "    \n",
    "    # Calculate wind speed and direction\n",
    "    speed = np.sqrt(u**2 + v**2)\n",
    "    wdir = 270 - np.rad2deg(np.arctan2(v,u))\n",
    "\n",
    "    # Make sure wind directions are not greater than 360 degrees\n",
    "    wdir = np.where(wdir >= 360, wdir-360, wdir)\n",
    "\n",
    "    # Calculate the residual of the retrieval solution\n",
    "    residual = (np.sqrt(np.nanmean(((((u*x)+(v*y)+((w*z))[None,:])\\\n",
    "                /np.sqrt(x**2+y**2+z**2))-vr)**2,axis = 0)))\n",
    "    \n",
    "    return u,v,w,speed,wdir,du,dv,dw,residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c7dec",
   "metadata": {},
   "source": [
    "We are going to loop over each scan in the VAD file. Before passing the data to the VAD fucntion, we are going to filter out gates that are below an intensity threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b692a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = np.where(intensity < 1.006)\n",
    "velocity[foo] = np.nan\n",
    "\n",
    "# Initialize the arrays for the retrieval\n",
    "u = np.ones((len(u_snum),len(rng)))*np.nan\n",
    "v = np.ones((len(u_snum),len(rng)))*np.nan\n",
    "w = np.ones((len(u_snum),len(rng)))*np.nan\n",
    "du = np.ones((len(u_snum),len(rng)))*np.nan\n",
    "dv = np.ones((len(u_snum),len(rng)))*np.nan\n",
    "dw = np.ones((len(u_snum),len(rng)))*np.nan\n",
    "speed = np.ones((len(u_snum),len(rng)))*np.nan\n",
    "wdir = np.ones((len(u_snum),len(rng)))*np.nan\n",
    "residual = np.ones((len(u_snum),len(rng)))*np.nan\n",
    "\n",
    "time = np.ones(len(u_snum))*np.nan\n",
    "count = 0\n",
    "# This is the loop that loops over each scan\n",
    "for i in range(len(u_snum)):\n",
    "    foo = np.where(u_snum[i] == snum)[0]\n",
    "    \n",
    "    time[i] = np.nanmean(hour[foo])\n",
    "    \n",
    "    # Just a counter to prove that it is working\n",
    "    if int(time[i]) >= count:\n",
    "        print('Processing scan at ' + str(time[i]))\n",
    "        count += 1\n",
    "        \n",
    "    u[i], v[i], w[i], speed[i], wdir[i],\\\n",
    "    du[i], dv[i], dw[i], residual[i] = ARM_VAD(velocity[foo,:], rng, elevation[foo][0], azimuth[foo])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb964d4",
   "metadata": {},
   "source": [
    "The VAD processing is done! It is that simple! Lets plot the wave that is the result of the retrievals for the two previous scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd4fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking a random scan\n",
    "foo = np.where(u_snum[750] == snum)\n",
    "\n",
    "# Now find the range closest to 500 m\n",
    "fah = np.where(np.nanmin(np.abs(rng-0.5)) == np.abs(rng-0.5))\n",
    "\n",
    "deg = np.arange(0,361)\n",
    "\n",
    "fit = np.sin(np.deg2rad(deg)) * np.cos(np.deg2rad(elevation[0])) * u[750,fah[0]] + \\\n",
    "      np.cos(np.deg2rad(deg)) * np.cos(np.deg2rad(elevation[0])) * v[750,fah[0]] + \\\n",
    "      np.sin(np.deg2rad(elevation[0])) * w[750,fah[0]]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(deg,fit,color='#ff7f0e')\n",
    "plt.scatter(azimuth[foo],velocity[foo[0],fah[0]])\n",
    "plt.xlim(0,365)\n",
    "plt.ylim(-10,10)\n",
    "plt.ylabel('Vr [m/s]')\n",
    "plt.xlabel('Deg')\n",
    "plt.title('Hour: %2.2f UTC       RMS: %2.2f' % (np.nanmean(hour[foo[0]]), residual[750,fah[0]]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e60d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking a random scan\n",
    "foo = np.where(u_snum[1862] == snum)\n",
    "\n",
    "# Now find the range closest to 500 m\n",
    "fah = np.where(np.nanmin(np.abs(rng-0.5)) == np.abs(rng-0.5))\n",
    "\n",
    "deg = np.arange(0,361)\n",
    "\n",
    "fit = np.sin(np.deg2rad(deg)) * np.cos(np.deg2rad(elevation[0])) * u[1862,fah[0]] + \\\n",
    "      np.cos(np.deg2rad(deg)) * np.cos(np.deg2rad(elevation[0])) * v[1862,fah[0]] + \\\n",
    "      np.sin(np.deg2rad(elevation[0])) * w[1862,fah[0]]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(deg,fit,color='#ff7f0e')\n",
    "plt.scatter(azimuth[foo],velocity[foo[0],fah[0]])\n",
    "plt.xlim(0,365)\n",
    "plt.ylim(-10,10)\n",
    "plt.ylabel('Vr [m/s]')\n",
    "plt.xlabel('Deg')\n",
    "plt.title('Hour: %2.2f UTC       RMS: %2.2f' % (np.nanmean(hour[foo[0]]), residual[1862,fah[0]]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d01b97f",
   "metadata": {},
   "source": [
    "As you can see, the RMS values of the daytime fit are slightly higher than the nighttime fit, but both retrivials are pretty good. It is always good to check out the RMS of the retrieval before using the data just to make sure the retrieval is decent.\n",
    "\n",
    "Now lets display the data! We are going to use some colormaps from the cmocean package. You might need to install the cmocean package for this code to work. For Anaconda this is just\n",
    "```\n",
    "conda install -c conda-forge cmocean\n",
    "```\n",
    "or with pip\n",
    "```\n",
    "pip install cmocean\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a28f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmocean\n",
    "\n",
    "# We need to calculate height from the range values\n",
    "hgt = np.sin(elevation[0])*rng\n",
    "\n",
    "fig, (wnd_spd, wnd_dir) =  plt.subplots(2, sharex=True)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "a = wnd_spd.pcolormesh(time,hgt,speed.T,cmap = 'gist_stern_r', vmin=0, vmax=25, shading = 'auto')\n",
    "b = wnd_dir.pcolormesh(time,hgt,wdir.T,cmap = cmocean.cm.phase, vmin=0,vmax=360, shading = 'auto')\n",
    "\n",
    "cb = plt.colorbar(a, ax=wnd_spd)\n",
    "cb.set_label('Wind Speed [m/s]')\n",
    "\n",
    "cb = plt.colorbar(b, ax=wnd_dir)\n",
    "cb.set_label('Wind Dir [deg]')\n",
    "    \n",
    "wnd_spd.set_ylim([0,2])\n",
    "wnd_dir.set_ylim([0,2])\n",
    "\n",
    "wnd_spd.set_ylabel('Height [km]')\n",
    "wnd_dir.set_ylabel('Height [km]')\n",
    "wnd_dir.set_xlabel('Hour [UTC]')\n",
    "\n",
    "wnd_spd.set_title('Wind Speed')\n",
    "wnd_dir.set_title('Wind Direction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0789862",
   "metadata": {},
   "source": [
    "Looks like there was a pretty strong LLJ on this night! Notice that the is still bad data when near the top of the lidar's range. This can be mitigated by making the intensity filter a little more stringent. The could, however, get rid of some good data as well. Also notice that the first heights are also bad data. This is due to those gates being in the lidar blind zone. \n",
    "\n",
    "You might also want to plot the RMS to get a sense of where the retrievals are good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f22ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (rms) =  plt.subplots(1)\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "a = rms.pcolormesh(time,hgt,residual.T,cmap = 'gist_stern_r', vmin=0, vmax=1, shading = 'auto')\n",
    "\n",
    "cb = plt.colorbar(a, ax=rms)\n",
    "cb.set_label('RMS [m/s]')\n",
    " \n",
    "rms.set_ylim([0,2])\n",
    "\n",
    "rms.set_ylabel('Height [km]')\n",
    "rms.set_xlabel('Hour [UTC]')\n",
    "\n",
    "rms.set_title('RMS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ba279f",
   "metadata": {},
   "source": [
    "This clearly shows how the retrivals get worse during the day.\n",
    "\n",
    "So there you go! You now can process PPI data from CLAMPS! If you have come this far you might be happy to know that 95% of the time you will never have to process your own VADs. The CLAMPS data system shoud do this for you. This processed data can also be found on the THREDDS server. We are going to open one of these processed VAD files and plot a wind profile for this day so you can get familar with the processed VAD files.\n",
    "\n",
    "First, lets grab the file. This is very similar to what we did previously for the unprocessed PPIs. The only changes to the previous code are to the catalog URL and the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f3adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catalog for the CLAMPS2 ingested VAD data\n",
    "catURL = \"https://data.nssl.noaa.gov/thredds/catalog/FRDD/CLAMPS/clamps/clamps2/processed/clampsdlvadC2.c1/catalog.xml\"\n",
    "\n",
    "# Open the catalog\n",
    "cat = TDSCatalog(catURL)\n",
    "\n",
    "# Date we want to grab\n",
    "dt = datetime(2021, 6, 20)\n",
    "\n",
    "# Get the dates for all the netCDF datasets (have to do it this way because file names can be messed up)\n",
    "nc_dates = []\n",
    "for ds in cat.datasets:\n",
    "    if '.cdf' in ds:\n",
    "        try:\n",
    "            nc_dates.append(datetime.strptime(ds, \"clampsdlvadC2.c1.%Y%m%d.%H%M%S.cdf\"))\n",
    "        except:\n",
    "            nc_dates.append(datetime(2100,1,1))\n",
    "            print('This file name is messed up: ' + ds)\n",
    "\n",
    "nc_dates = np.array(nc_dates)\n",
    "\n",
    "# Find the index of the date we want\n",
    "ind = np.argmin(np.abs(dt - nc_dates))\n",
    "\n",
    "# Get the dataset\n",
    "ds = cat.datasets[ind]\n",
    "\n",
    "# Download the dataset we identified to our current working directory\n",
    "ds.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947830ea",
   "metadata": {},
   "source": [
    "Open the file and print out the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0664f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = Dataset(ds,'r')\n",
    "\n",
    "print(nc.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e1e064",
   "metadata": {},
   "source": [
    "Looks like we want the hour, wspd, wdir, height, and intensity (for filtering) fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c6cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour = nc.variables['hour'][:]\n",
    "wspd = nc.variables['wspd'][:]\n",
    "wdir = nc.variables['wdir'][:]\n",
    "hgt = nc.variables['height'][:]\n",
    "intensity = nc.variables['intensity'][:]\n",
    "nc.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e03f97",
   "metadata": {},
   "source": [
    "Now filter the data with intensity, calculate u and v from the wind speed and wind direction variables and plot a wind profile for a random time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cb28b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = np.where(intensity < 1.006)\n",
    "wspd[foo] = np.nan\n",
    "wdir[foo] = np.nan\n",
    "\n",
    "u = -wspd*np.sin(np.deg2rad(wdir))\n",
    "v = -wspd*np.cos(np.deg2rad(wdir))\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(u[600],hgt)\n",
    "plt.xlim(-20,20)\n",
    "plt.ylim(0,2)\n",
    "plt.xlabel('U [m/s]')\n",
    "plt.ylabel('Height [km]')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(v[600],hgt)\n",
    "plt.xlim(-20,20)\n",
    "plt.ylim(0,2)\n",
    "plt.xlabel('V [m/s]')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a05c955",
   "metadata": {},
   "source": [
    "That wasn't too bad! As an exercise feel free to try to make a time-height plot from the processed VAD file or if you are feeling bold, download data from another day and play around with that. Maybe even try to process PPI files for different day just to get more familar with the process. With over 5 years of CLAMPS data you should be able to find something interesting!\n",
    "\n",
    "Once you feel comfortable with VADs, check out the tutorials for working with vertical stare data, and RHIs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c7a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this space to have fun with data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
